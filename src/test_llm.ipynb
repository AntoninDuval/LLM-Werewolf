{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "# OpenAI model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "class LangChainHandler:\n",
    "    def __init__(self, model=\"gpt-4\", api_key=None, local_model_name=None):\n",
    "        \"\"\"\n",
    "        Initialize the LangChainHandler.\n",
    "        \n",
    "        Args:\n",
    "            model (str): The model to use (\"gpt-4\" for OpenAI, or local Hugging Face model).\n",
    "            api_key (str): OpenAI API key (if using OpenAI model).\n",
    "            local_model_name (str): Local model name for Hugging Face (if using local model).\n",
    "        \"\"\"\n",
    "        if api_key:\n",
    "            self.llm = ChatOpenAI(model=model, openai_api_key=api_key)\n",
    "        elif local_model_name:\n",
    "            \n",
    "            from langchain.llms import HuggingFacePipeline\n",
    "            from transformers import pipeline\n",
    "            local_pipeline = pipeline(\"text-generation\", model=local_model_name)\n",
    "            self.llm = HuggingFacePipeline(pipeline=local_pipeline)\n",
    "        else:\n",
    "            raise ValueError(\"Either `api_key` or `local_model_name` must be provided.\")\n",
    "\n",
    "    def generate_response(self, prompt_template: str, variables: dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using LangChain.\n",
    "        \n",
    "        Args:\n",
    "            prompt_template (str): The template for the prompt.\n",
    "            variables (dict): The variables to fill in the prompt.\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(input_variables=list(variables.keys()), template=prompt_template)\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        response = chain.run(variables)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_23936\\2019992008.py:42: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=self.llm, prompt=prompt)\n",
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_23936\\2019992008.py:43: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: Hey everyone! I think it’s a bit early to pin the blame on anyone, especially Zack. We should discuss and gather more information before making any accusations. Let’s share our thoughts and observations about each other’s behavior. What do you all think?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the .env file.\")\n",
    "\n",
    "openai_handler = LangChainHandler(model=\"gpt-4o-mini\",\n",
    "                                   api_key=openai_api_key)\n",
    "\n",
    "\n",
    "# Local Hugging Face model\n",
    "# local_handler = LangChainHandler(local_model_name=\"gpt2\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are {player_name}, an AI player in a Werewolf-themed game. The current state of the game is as follows:\n",
    "\n",
    "- **Current Phase**: {current_phase} Phase\n",
    "- **Current Day**: Day {current_day}\n",
    "- **Alive Players**: {alive_players}\n",
    "- **Dead Players**: {dead_players}\n",
    "- **Game Over**: {game_over}\n",
    "- **Winner**: {winner}\n",
    "\n",
    "Here is the **Chat History** of the conversation so far:\n",
    "{formatted_chat_history}\n",
    "\n",
    "Your Role: {player_role}\n",
    "\n",
    "### Rules for Your Response:\n",
    "1. If you want to contribute to the conversation, compose a message and send it.\n",
    "2. If you don't want to say anything right now, wait silently.\n",
    "\n",
    "### Goals Based on Your Role:\n",
    "- **Werewolf**: Be sneaky, avoid suspicion, and manipulate the villagers.\n",
    "- **Villager**: Be cooperative, identify the werewolves, and convince others to trust you.\n",
    "\n",
    "Compose a response that aligns with your role's goals or wait silently for now.\n",
    "\"\"\"\n",
    "\n",
    "state_summary = {\n",
    "    \"current_phase\": \"Debate\",\n",
    "    \"current_day\": 1,\n",
    "    \"alive_players\": \"Dudu, Ivan, Michael, John, Zack\",\n",
    "    \"dead_players\": '',\n",
    "    \"chat_history\": [{'day': 1, 'phase': 'Day', 'sender': 'Dudu', 'text': 'Hi !'}, \n",
    "                      {'day': 1, 'phase': 'Day', 'sender': 'Ivan', 'text': 'Hello everyone'}, \n",
    "                      {'day': 1, 'phase': 'Day', 'sender': 'John', 'text': 'I m sure Zack is a Werewolf. We should vote for him'}],\n",
    "    \"game_over\": False,\n",
    "    \"winner\": None,\n",
    "    \"player_name\": \"Zach\",\n",
    "    \"player_role\": \"Villager\"\n",
    "}\n",
    "\n",
    "def format_chat_history(chat_history):\n",
    "    return \"\\n\".join([f\"Day {entry['day']} ({entry['phase']} Phase) - {entry['sender']}: {entry['text']}\" for entry in chat_history])\n",
    "\n",
    "state_summary['formatted_chat_history'] = format_chat_history(state_summary['chat_history'])\n",
    "\n",
    "state_summary\n",
    "\n",
    "# # Generate a response using OpenAI\n",
    "response = openai_handler.generate_response(prompt_template, state_summary)\n",
    "print(\"OpenAI Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-werewolf-Q4pjDCtd-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
