{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "class LangChainHandler:\n",
    "    def __init__(self, model=\"gpt-4\", api_key=None, local_model_name=None):\n",
    "        \"\"\"\n",
    "        Initialize the LangChainHandler.\n",
    "        \n",
    "        Args:\n",
    "            model (str): The model to use (\"gpt-4\" for OpenAI, or local Hugging Face model).\n",
    "            api_key (str): OpenAI API key (if using OpenAI model).\n",
    "            local_model_name (str): Local model name for Hugging Face (if using local model).\n",
    "        \"\"\"\n",
    "        if api_key:\n",
    "            self.llm = ChatOpenAI(model=model, openai_api_key=api_key)\n",
    "        elif local_model_name:\n",
    "            local_pipeline = pipeline(\"text-generation\", model=local_model_name)\n",
    "            self.llm = HuggingFacePipeline(pipeline=local_pipeline)\n",
    "        else:\n",
    "            raise ValueError(\"Either `api_key` or `local_model_name` must be provided.\")\n",
    "\n",
    "    def generate_response(self, prompt_template: str, variables: dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using LangChain.\n",
    "        \n",
    "        Args:\n",
    "            prompt_template (str): The template for the prompt.\n",
    "            variables (dict): The variables to fill in the prompt.\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(input_variables=list(variables.keys()), template=prompt_template)\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        response = chain.run(variables)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: Day 1 (Day Phase) - Zack: Hey everyone! I think it's important we don't jump to conclusions too quickly. John, do you have any reasons for suspecting me? It would be helpful to hear everyone's thoughts so we can figure out who the werewolves might be together. Letâ€™s share our observations!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI model\n",
    "openai_handler = LangChainHandler(model=\"gpt-4o-mini\",\n",
    "                                   api_key=\"\")\n",
    "\n",
    "\n",
    "# Local Hugging Face model\n",
    "# local_handler = LangChainHandler(local_model_name=\"gpt2\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are {player_name}, an AI player in a Werewolf-themed game. The current state of the game is as follows:\n",
    "\n",
    "- **Current Phase**: {current_phase} Phase\n",
    "- **Current Day**: Day {current_day}\n",
    "- **Alive Players**: {alive_players}\n",
    "- **Dead Players**: {dead_players}\n",
    "- **Game Over**: {game_over}\n",
    "- **Winner**: {winner}\n",
    "\n",
    "Here is the **Chat History** of the conversation so far:\n",
    "{formatted_chat_history}\n",
    "\n",
    "Your Role: {player_role}\n",
    "\n",
    "### Rules for Your Response:\n",
    "1. If you want to contribute to the conversation, compose a message and send it.\n",
    "2. If you don't want to say anything right now, wait silently.\n",
    "\n",
    "### Goals Based on Your Role:\n",
    "- **Werewolf**: Be sneaky, avoid suspicion, and manipulate the villagers.\n",
    "- **Villager**: Be cooperative, identify the werewolves, and convince others to trust you.\n",
    "\n",
    "Compose a response that aligns with your role's goals or wait silently for now.\n",
    "\"\"\"\n",
    "\n",
    "state_summary = {\n",
    "    \"current_phase\": \"Debate\",\n",
    "    \"current_day\": 1,\n",
    "    \"alive_players\": \"Dudu, Ivan, Michael, John, Zack\",\n",
    "    \"dead_players\": '',\n",
    "    \"chat_history\": [{'day': 1, 'phase': 'Day', 'sender': 'Dudu', 'text': 'Hi !'}, \n",
    "                      {'day': 1, 'phase': 'Day', 'sender': 'Ivan', 'text': 'Hello everyone'}, \n",
    "                      {'day': 1, 'phase': 'Day', 'sender': 'John', 'text': 'I m sure Zack is a Werewolf. We should vote for him'}],\n",
    "    \"game_over\": False,\n",
    "    \"winner\": None,\n",
    "    \"player_name\": \"Zach\",\n",
    "    \"player_role\": \"Villager\"\n",
    "}\n",
    "\n",
    "def format_chat_history(chat_history):\n",
    "    return \"\\n\".join([f\"Day {entry['day']} ({entry['phase']} Phase) - {entry['sender']}: {entry['text']}\" for entry in chat_history])\n",
    "\n",
    "state_summary['formatted_chat_history'] = format_chat_history(state_summary['chat_history'])\n",
    "\n",
    "state_summary\n",
    "\n",
    "# # Generate a response using OpenAI\n",
    "response = openai_handler.generate_response(prompt_template, state_summary)\n",
    "print(\"OpenAI Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
