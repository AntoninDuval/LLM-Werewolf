import openai
from transformers import pipeline

class LLMHandler:
    def __init__(self, mode="openai", model_name=None, api_key=None):
        """
        Initialize the LLMHandler.

        Args:
            mode (str): The mode of operation. Either "openai" for OpenAI API or "local" for transformers.
            model_name (str, optional): The name of the model (for local mode). Defaults to None.
            api_key (str, optional): The API key (for OpenAI mode). Defaults to None.
        """
        self.mode = mode
        self.model_name = model_name
        self.api_key = api_key

        if mode == "local" and model_name:
            self.local_pipeline = pipeline("text-generation", model=model_name)
        elif mode == "openai" and api_key:
            openai.api_key = api_key
        else:
            raise ValueError("Invalid configuration. Please specify mode and required parameters.")

    def get_response(self, prompt: str, max_tokens: int = 100):
        """
        Generate a response using the selected LLM.

        Args:
            prompt (str): The input prompt for the LLM.
            max_tokens (int): The maximum number of tokens for the response.

        Returns:
            str: The response generated by the LLM.
        """
        if self.mode == "openai":
            return self._get_response_openai(prompt, max_tokens)
        elif self.mode == "local":
            return self._get_response_local(prompt, max_tokens)
        else:
            raise ValueError("Invalid mode. Use 'openai' or 'local'.")

    def _get_response_openai(self, prompt: str, max_tokens: int):
        """
        Generate a response using OpenAI's GPT API.

        Args:
            prompt (str): The input prompt.
            max_tokens (int): The maximum number of tokens.

        Returns:
            str: The response.
        """
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",  # Use gpt-4 or gpt-3.5-turbo depending on your access
                messages=[{"role": "system", "content": "You are an intelligent agent in a Werewolf game."},
                          {"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"Error using OpenAI API: {e}")
            return "Error: Unable to generate response."

    def _get_response_local(self, prompt: str, max_tokens: int):
        """
        Generate a response using a local LLM with Hugging Face's transformers.

        Args:
            prompt (str): The input prompt.
            max_tokens (int): The maximum number of tokens.

        Returns:
            str: The response.
        """
        try:
            responses = self.local_pipeline(prompt, max_length=max_tokens, num_return_sequences=1)
            return responses[0]["generated_text"].strip()
        except Exception as e:
            print(f"Error using local model: {e}")
            return "Error: Unable to generate response."
